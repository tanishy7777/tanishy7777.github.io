<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="tanishyelgoe.tech/feed.xml" rel="self" type="application/atom+xml"/><link href="tanishyelgoe.tech/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-23T09:58:35+00:00</updated><id>tanishyelgoe.tech/feed.xml</id><title type="html">blank</title><subtitle>My personal website </subtitle><entry><title type="html">Single Linkage Agglomerative Clustering</title><link href="tanishyelgoe.tech/blog/2025/Single-Linkage-Clustering/" rel="alternate" type="text/html" title="Single Linkage Agglomerative Clustering"/><published>2025-02-10T16:40:00+00:00</published><updated>2025-02-10T16:40:00+00:00</updated><id>tanishyelgoe.tech/blog/2025/Single-Linkage-Clustering</id><content type="html" xml:base="tanishyelgoe.tech/blog/2025/Single-Linkage-Clustering/"><![CDATA[<p>The goal of this blog post is to understand how the Single Linkage Agglomerative Clustering works. To understand this, lets start with something more fundamental. What is <strong>Clustering</strong>?</p> <h2 id="clustering">Clustering</h2> <p>Lets start with a fun analogy to understand clustering and then formalize it. Lets say you have a basket of chips, unfortunately for you the chips of all the different flavors are mixed together. Being the perfectionist you are, you want to <strong>‚Äúcluster‚Äù</strong> them according to their flavor. To ease your task you have been given access to the <em>‚ÄúCHIPS-CLASSIFICATOR-9281RX‚Äù</em> by the generous folks at IIT Gandhinagar, you can place the chips in it and it looks at the features of each chip and analyzes its <strong>‚Äúfeatures‚Äù</strong> and pumps out an output label telling you what kind of chip flavor it is.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/clustering/clustering_analogy-480.webp 480w,/assets/img/clustering/clustering_analogy-800.webp 800w,/assets/img/clustering/clustering_analogy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/clustering/clustering_analogy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An image of the chips analogy </div> <p>More formally, in machine learning Clustering is an unsupervised problem where given the ‚Äúinput data‚Äù we have to partition it according to the ‚Äúfeatures‚Äù of the data. This can also be said in another way Clustering is the task of grouping data in such a way that data points in the same cluster are more similar (this similarity metric is decided by you, an example is the euclidean distance) to each other than to those in other clusters.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" width="400px" height="300px"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/clustering/clustering_example-480.webp 480w,/assets/img/clustering/clustering_example-800.webp 800w,/assets/img/clustering/clustering_example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/clustering/clustering_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An example of clustering(6 clusters) based on euclidean distances </div> <h2 id="single-linkage-agglomerative-custering">Single Linkage Agglomerative Custering</h2> <p>Great now that you know what clustering is(or maybe you already did), lets start with what the title promised, <strong>Single Linkage Agglomerative Clustering</strong>.</p> <p>The main idea is to ‚Äúkeep merging closest set of data points, building larger clusters out of smaller ones‚Äù, this way we end up with a lot of different clusterings. If you know what hiererchical clustering, Single Linkage Clustering is a kind of hiererchical clustering.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Algo:
    Keep a current list of clusters
    Initially, each point in its own cluster
    while #clusters &gt; 1:
          choose closest pair
          merge them,
          update list by deleting these and adding new cluster
</code></pre></div></div> <h3 id="why-bother-with-this">Why bother with this?</h3> <p>Why do we need yet another algorithm if we already have kmeans? To answer that question lets look at the following example:</p> <p><img src="/assets/img/clustering/clustering_need.png" alt="Example where k means struggles"/></p> <p>For data like this where the idea clustering dosent necesssarily have ‚Äúballs in high dim‚Äù algorithms like k means, k median struggle. In this case Single linkage clustering is easily able to cluster the data. This is because it dosent assume that the data will be distributed around the center of the cluster(ball shape). Instead, since it merges the clusters that are closer to each other first, it can adapt to a wider range of shapes of data.</p> <p>Another reason why this algorithm might be useful is when we dont know the target number of clusters(hierechichal clusters are especially useful in this case), we can create a family of clusters for each k</p> <h3 id="how-to-define-closeness-of-two-clusters">How to define closeness of two clusters?</h3> <p>There are several ways to define closeness between two clusters. For example,</p> \[\begin{aligned} &amp;\text{closest pair: single linkage clustering} \quad d_1(C, C') = \min_{x \in C, y \in C'} d(x, y) \\ &amp;\text{Furthest pair: complete link clustering} \quad d_2(C, C') = \max_{x \in C, y \in C'} d(x, y) \\ &amp;\text{Average of all pairs: average link} \quad d_3(C, C') = \text{avg}_{x \in C, y \in C'} d(x, y) \end{aligned}\] <p><img src="/assets/img/clustering/clustering_hiererchical.png" alt="Dendograph of hierechical clustering"/></p> <h3 id="cost-of-single-linkage-clustering">Cost of Single Linkage Clustering</h3> <p>Single Linkage Clustering essentially creates clusters ${C_1, C_2,‚Ä¶, C_k}$ from the data that maximizes the minimum inter-cluster distance maximize ${Cost_k(C)}$ ùëêùëúùë†ùë°_ùëò (ùê∂) = $\min d_{C_i, C_j}(C_i, C_j)$</p> <p>Basically, assume among all the clusters the minimum distance between any 2 clusters is ‚Äúd_min‚Äù, then it tries to maximize this.</p> <p><img src="/assets/img/clustering/clustering_cost.png" alt="To illustrate the cost"/></p> <p>As shown in the what the Single Linkage cost, tries to do is, maximize the cost where, cost = min(d1,d2,d3)</p> <h3 id="properties-of-single-linkage-clustering">Properties of Single Linkage Clustering</h3> <ul> <li>Effective in low dimension but might struggle in higher dimension due to ‚Äúcurse of dimensionality‚Äù</li> <li>Usually needs $O(n^2)$ space</li> </ul> <h3 id="examples-where-single-linkage-might-fail">Examples where Single Linkage might fail</h3> <p>TODO</p> <h3 id="mnist-example-single-linkage-clustering">MNIST example: Single Linkage Clustering</h3>]]></content><author><name></name></author><category term="algorithms"/><category term="algorithms"/><summary type="html"><![CDATA[A brief post describing how the Single Linkage Agglomerative Clustering works]]></summary></entry><entry><title type="html">Dynamic Time Warping</title><link href="tanishyelgoe.tech/blog/2025/DTW_and_DTW_GI/" rel="alternate" type="text/html" title="Dynamic Time Warping"/><published>2025-02-10T16:40:00+00:00</published><updated>2025-02-10T16:40:00+00:00</updated><id>tanishyelgoe.tech/blog/2025/DTW_and_DTW_GI</id><content type="html" xml:base="tanishyelgoe.tech/blog/2025/DTW_and_DTW_GI/"><![CDATA[<p>In this blog I want to talk about a similarity measures which is very useful for comparing time series data.<br/> This is Dynamic Time Warping (DTW) [1] Let‚Äôs explore what it is.</p> <h2 id="why-do-we-need-it">Why do we need it?</h2> <p>First of all, as always, let‚Äôs try to understand why we even need a new fancy similarity measure like DTW.<br/> The reason is that while simple similarity measures might fail for temporal sequences that differ in speed from each other, DTW shines.</p> <p>For example:</p> <p>For example: A = [0,1,0,1,0,1,0,1,0,1,0,1] B = [0,0,1,1,0,0,1,1,0,0,1,1]</p> <p><img src="/assets/img/dtw/dtw_example.png" alt="DTW Example"/></p> <p>You can observe that these two signals have<br/> Euclidean distance = 7, but if we calculate the DTW distance (will be covered later :D) it will be zero!</p> <p>This might seem surprising at first but if you carefully observe the two signals it makes sense since they are quite similar.</p> <p>We can see that A is just a sped up version of signal B.<br/> \(\text{Timeperiod of B} = 2 \times \text{Timeperiod of A}\)</p> <p>Another example comparing DTW and Eucledian Distance as a similarity measure <img src="/assets/img/dtw/thumbnail.png" alt="Example"/></p> <h2 id="formal-definition">Formal Definition</h2> <p>Now that we have gotten some intuition about DTW, let‚Äôs look at the mathematical definition of DTW.</p> <p>Let $ x $ and $ y $ be two time series of varying lengths such that<br/> $ x \in \mathbb{R}^{n \times d} $ and $ y \in \mathbb{R}^{m \times d} $ or</p> \[x = \{ x_1, x_2, \dots, x_n \}, \quad y = \{ y_1, y_2, \dots, y_m \}\] <p>where $ x_i $ and $ y_i $ are $ d $-dimensional vectors.</p> <p>Then,</p> \[DTW(x, y) = \min_{\pi} \sqrt{\sum_{(i,j) \in \pi} d(x_i, y_j)^2}\] <p>where $\pi = [\pi_0, \dots, \pi_K]$ is a path that satisfies the following properties:</p> <ul> <li>It is a list of index pairs $ \pi_k = (i_k, j_k) $ with $ 0 \leq i_k &lt; n $ and $ 0 \leq j_k &lt; m $.</li> <li>$ \pi_0 = (0, 0) $ and $ \pi_K = (n-1, m-1) $.</li> <li>For all $ k &gt; 0 $, $ \pi_k = (i_k, j_k) $ is related to $ \pi_{k-1} = (i_{k-1}, j_{k-1}) $ as follows: <ul> <li>$ i_{k-1} \leq i_k \leq i_{k-1} + 1 $</li> <li>$ j_{k-1} \leq j_k \leq j_{k-1} + 1 $</li> </ul> </li> </ul> <p>Let‚Äôs simplify this: what all this means is that first we construct an $ n \times m $ grid/matrix, say $ M $, where $ M_{i,j} $ denotes the Euclidean distance between $ x_i $ and $ y_j $.</p> <p>After this we find the cumulative distances using dynamic programming. The idea is to find the shortest <strong>alignment</strong> path between the top left point $ M_{0,0} $ and the bottom right point $ M_{n-1, m-1} $. This means that we can only move in the directions $ (i+1, j) $, $ (i, j+1) $, or $ (i+1, j+1) $.<br/> This can be achieved using DP in $ O(n^2) $ time complexity.</p> <p>If you are unfamiliar with DP, don‚Äôt worry ‚Äì it will get clear.<br/> In a grid, how do you find the shortest <strong>alignment</strong> path between the top left and the bottom right point?<br/> The idea is to use a greedy strategy where you keep track of the cumulative sum until the current point $ M_{i,j} $ and check the minimum increment in distance when you go to $ (M_{i+1,j}, M_{i,j+1}, M_{i+1,j+1}) $.<br/> Since $ M_{i,j} \geq 0 $ for all $ i, j $, this will work.</p> <p>Note: We are finding the shortes <strong>alignment</strong> path not the shortest path</p> <p>Now, DP just provides us a way to cache the cumulative sum so that we don‚Äôt have to start from the base case every time for a point $ M_{i,j} $.</p> <p>Basically,</p> \[DP(i,j,M) = M_{i,j} + \min\{DP(i-1,j), \; DP(i,j-1), \; DP(i-1,j-1)\}\] <p>On the left, the $ M $ matrix is shown, and on the right the DP matrix is shown.</p> <p><img src="/assets/img/dtw/distance_matrix.png" alt="Distance Matrix"/></p> <p>The 11 circled in green in the right matrix is the shortest alignment distance(DTW distance) between $ M_{0,0} $ and $ M_{n-1, m-1} $.</p> <h2 id="efficient-dtw">Efficient DTW</h2> <p>$ O(nm) $ might be a problem if the time series is huge. For such cases there are 2 techniques that can help make the search for the shortest alignment path easier.</p> <h3 id="sakoe-chiba-band">Sakoe Chiba Band</h3> <p>Instead of searching in the entire grid, we can create a bounding mask, i.e., restrict the search space. One of the techniques to do this is the Sakoe Chiba band [3].<br/> Imagine placing a tube along the diagonal. This is exactly what the Sakoe Chiba band is. Now the width of this tube can be controlled by a parameter, say <code class="language-plaintext highlighter-rouge">window</code>.</p> <p><img src="/assets/img/dtw/SakoeChiba.png" alt="Sakoe Chiba Band"/></p> <p>If <code class="language-plaintext highlighter-rouge">window = 0.1</code> then that means at each time step $ i $, the alignment can only shift within ¬±10% of the total sequence length. That means for $ DTW(i, j) $, the value of $ j $ must be within: \(i - 10 \leq j \leq i + 10\)</p> <h3 id="itakura-max-slope">Itakura Max Slope</h3> <p>This controls how slanted the tube is. For example, if you want the search space to be something like this:</p> <p><img src="/assets/img/dtw/Itakura.png" alt="Itakura Slope"/></p> <p>Note: There can be other kinds of bounding boxes; you can even create your own bounding box mask as well. Sakoe Chiba band is just an example which is widely used.</p> <h2 id="optimal-alignment-path">Optimal Alignment Path</h2> <p>After the DP matrix has been calculated, we can then backtrack in linear time $ O(n + m) $ because in each step, we move either:</p> <ul> <li>Diagonally $(i-1, j-1)$,</li> <li>Up $(i-1, j)$, or</li> <li>Left $(i, j-1)$, based on the minimum cost direction.</li> </ul> <p><img src="/assets/img/dtw/Path.png" alt="Optimal Alignment Path"/><br/> <em>Example of Optimal Alignment Path</em></p> <p><strong>Note:</strong><br/> Dynamic Time Warping holds the following properties:</p> <ul> <li>$ \forall x, y, \; DTW(x, y) \geq 0 $</li> <li>$ \forall x, \; DTW(x, x) = 0 $</li> </ul> <p>However, mathematically speaking, DTW is not a valid distance since it does not satisfy the triangular inequality.</p> <hr/> <h2 id="references">References</h2> <ol> <li>[1] <a href="https://en.wikipedia.org/wiki/Dynamic_time_warping">Dynamic Time Warping (DTW) Overview</a></li> <li>[2] T. Vayer, R. Tavenard, L. Chapel, N. Courty, R. Flamary, and Y. Soullard, ‚ÄúTime Series Alignment with Global Invariances,‚Äù arXiv.org, 2020. https://arxiv.org/abs/2002.03848 (accessed Feb. 23, 2025).</li> <li> <p>[3] H. Sakoe and S. Chiba, ‚ÄúDynamic programming algorithm optimization for spoken word recognition,‚Äù IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, no. 1, pp. 43‚Äì49, Feb. 1978, doi: https://doi.org/10.1109/tassp.1978.1163055.</p> </li> <li>Romain Tavenard, ‚ÄúDTW with Global Invariances,‚Äù Github.io, Dec. 17, 2020. https://rtavenar.github.io/hdr/parts/01/dtw/dtw_gi.html (accessed Feb. 23, 2025).</li> <li>‚Äúdtw_distance,‚Äù aeon, 2025. https://www.aeon-toolkit.org/en/v1.0.0/api_reference/auto_generated/aeon.distances.dtw_distance.html (accessed Feb. 23, 2025).</li> <li>‚ÄúDynamic Time Warping ‚Äî tslearn 0.5.2 documentation,‚Äù tslearn.readthedocs.io. https://tslearn.readthedocs.io/en/stable/user_guide/dtw.html</li> <li>Y. Li, R. W. Liu, Z. Liu, and J. Liu, ‚ÄúSimilarity Grouping-Guided Neural Network Modeling for Maritime Time Series Prediction,‚Äù May 13, 2019. https://www.researchgate.net/publication/333077403_Similarity_Grouping-Guided_Neural_Network_Modeling_for_Maritime_Time_Series_Prediction</li> </ol> <p>‚Äå</p> <p>‚Äå</p> <p>‚Äå</p>]]></content><author><name></name></author><category term="algorithms"/><category term="algorithms"/><summary type="html"><![CDATA[A brief post describing how the Dynamic Time Warping works]]></summary></entry><entry><title type="html">Graham Scan Finding the Convex Hull in Object Detection</title><link href="tanishyelgoe.tech/blog/2025/graham-scan-convex-hull-algorithm/" rel="alternate" type="text/html" title="Graham Scan Finding the Convex Hull in Object Detection"/><published>2025-02-10T16:32:00+00:00</published><updated>2025-02-10T16:32:00+00:00</updated><id>tanishyelgoe.tech/blog/2025/graham-scan-convex-hull-algorithm</id><content type="html" xml:base="tanishyelgoe.tech/blog/2025/graham-scan-convex-hull-algorithm/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Hey there! Ever wondered how we can wrap a tight boundary around a bunch of points, like tracing the edges of a cloud of stars in the night sky? That‚Äôs essentially what the <strong>Graham Scan algorithm</strong> does‚Äîit finds the smallest convex polygon that contains all the given points, called the <strong>convex hull</strong>.</p> <p>In object detection, this is super useful for finding <strong>bounding boxes</strong> that are more accurate than simple rectangular boxes. So, let‚Äôs dive into how it works!</p> <hr/> <h2 id="what-is-a-convex-hull">What is a Convex Hull?</h2> <p>Imagine placing a rubber band around a set of nails on a board. When you release it, the rubber band snaps to form a tight boundary around the nails. This boundary is the convex hull‚Äîa polygon where every interior angle is less than 180 degrees.</p> <p>In mathematical terms, given a set of points $ P = {p_1, p_2, \dots, p_n} $, the convex hull is the smallest convex polygon that encloses all the points.</p> <hr/> <h2 id="the-graham-scan-algorithm">The Graham Scan Algorithm</h2> <p>The Graham Scan algorithm finds the convex hull in <strong>$O(n \log n)$</strong> time. Here‚Äôs the step-by-step breakdown:</p> <ol> <li><strong>Find the point with the lowest y-coordinate</strong> (break ties by choosing the leftmost point). This will be our starting point $ p_0 $.</li> <li><strong>Sort the remaining points by polar angle</strong> relative to $ p_0 $.</li> <li><strong>Traverse the sorted points and construct the hull</strong> using a stack: <ul> <li>Push the first two points onto the stack.</li> <li>For each subsequent point: <ul> <li>While the angle formed by the top two points and the current point makes a <strong>right turn</strong>, pop the top point from the stack.</li> <li>Push the current point onto the stack.</li> </ul> </li> </ul> </li> <li>The points remaining in the stack form the convex hull.</li> </ol> <hr/> <h2 id="math-behind-it">Math Behind It</h2> <p>To determine if three points $ p_1 $, $ p_2 $, and $ p_3 $ make a left or right turn, we use the <strong>cross product</strong> of the vectors:</p> \[\text{Cross Product} = (x_2 - x_1)(y_3 - y_1) - (y_2 - y_1)(x_3 - x_1)\] <ul> <li>If the result is <strong>positive</strong>, the points make a <strong>left turn</strong>.</li> <li>If it‚Äôs <strong>negative</strong>, they make a <strong>right turn</strong>.</li> <li>If it‚Äôs <strong>zero</strong>, the points are collinear.</li> </ul> <hr/> <h2 id="why-use-this-in-object-detection">Why Use This in Object Detection?</h2> <p>In object detection, keypoints often define the boundaries of an object. Using a convex hull instead of a simple bounding box can provide a tighter, more accurate boundary around irregularly shaped objects. This can be crucial for applications like:</p> <ul> <li>Autonomous driving (detecting pedestrians and vehicles)</li> <li>Medical imaging (outlining tumors or organs)</li> <li>Robotics (object grasping and manipulation)</li> </ul> <hr/> <h2 id="python-implementation">Python Implementation</h2> <p>Here‚Äôs a Python snippet to implement the Graham Scan algorithm:</p> <p>```python def cross_product(o, a, b): return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])</p> <p>def graham_scan(points): points = sorted(points) lower, upper = [], []</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for p in points:
    while len(lower) &gt;= 2 and cross_product(lower[-2], lower[-1], p) &lt;= 0:
        lower.pop()
    lower.append(p)

for p in reversed(points):
    while len(upper) &gt;= 2 and cross_product(upper[-2], upper[-1], p) &lt;= 0:
        upper.pop()
    upper.append(p)

return lower[:-1] + upper[:-1]
</code></pre></div></div> <h1 id="example-usage">Example usage</h1> <p>points = [(0, 0), (1, 1), (2, 2), (2, 0), (2, 4), (3, 3), (4, 2)] hull = graham_scan(points) print(‚ÄúConvex Hull:‚Äù, hull)</p>]]></content><author><name></name></author><category term="algorithms"/><category term="algorithms"/><summary type="html"><![CDATA[Learn how the Graham Scan algorithm works to find the convex hull of keypoints in object detection with fun explanations and math]]></summary></entry></feed>